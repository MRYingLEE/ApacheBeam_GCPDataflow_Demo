{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BeamOnColab DEMO.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9982NiHmlog"
      },
      "source": [
        "# Data Engineer coding task\r\n",
        "Convert level 3 to level 1 data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKKEVJyLp_0z"
      },
      "source": [
        "# Why Beam? Beam=Batch+strEAM\r\n",
        "\r\n",
        "Apache Beam (https://beam.apache.org/): An advanced unified programming model\r\n",
        "\r\n",
        "Implement batch and streaming data processing jobs that run on any execution engine.\r\n",
        "\r\n",
        "Apache Beam supports distributed processing back-ends, which include Apache Flink, Apache Spark, and Google Cloud Dataflow.\r\n",
        "\r\n",
        "So that Apache Beam application can be deployed on Google Cloud Dataflow as a SERVERLESS solution.\r\n",
        "\r\n",
        "Also, Apache Beam batch application can work in Colab.\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Mvl11eFmyBz"
      },
      "source": [
        "# Batch implementation\r\n",
        "\r\n",
        "In order to provide a **Serverless** solution, this task was completed as a notebook. \r\n",
        "This notebook was developed and tested on Colab (https://colab.research.google.com/) along with an Apache Beam DirectRunner.\r\n",
        "\r\n",
        " <font color=red>Colab will raise some errors for the Apache beam installation. Please ignore them, which won't affect the following functions.</font> "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GsjNKX11nVip",
        "outputId": "b87fd62d-f217-4a2a-ceda-8616f5b5a7bd"
      },
      "source": [
        "# Run and print a shell command.\r\n",
        "def run(cmd):\r\n",
        "  print('>> {}'.format(cmd))\r\n",
        "  !{cmd}\r\n",
        "  print('')\r\n",
        "\r\n",
        "# Install apache-beam.\r\n",
        "run('pip install --quiet apache-beam')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">> pip install --quiet apache-beam\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sh2c91HFpD1j"
      },
      "source": [
        "## Data Source\r\n",
        "\r\n",
        "The original data was upload to Google Cloud Storage in advance, which saves time to provide data.\r\n",
        "\r\n",
        "We are to download the data to the local."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UciiAE_TmggJ",
        "outputId": "4823139a-5942-4ec6-dadf-e6a11300fb50"
      },
      "source": [
        "run('gsutil cp gs://level3-to-level1-data/market_data_v2.csv .')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">> gsutil cp gs://level3-to-level1-data/market_data_v2.csv .\n",
            "Copying gs://level3-to-level1-data/market_data_v2.csv...\n",
            "- [1 files][  2.6 MiB/  2.6 MiB]                                                \n",
            "Operation completed over 1 objects/2.6 MiB.                                      \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jwF8sSIytsYA",
        "outputId": "7874a630-b7e6-4acd-f9c5-ba32dabaea57"
      },
      "source": [
        "!ls -l"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 4680\n",
            "-rw-r--r-- 1 root root 2026912 Dec 13 09:38 L1_market_data-00000-of-00001.csv\n",
            "-rw-r--r-- 1 root root 2757472 Dec 13 14:00 market_data_v2.csv\n",
            "drwxr-xr-x 1 root root    4096 Dec  2 22:04 sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZJqj4HBqHTW"
      },
      "source": [
        "## The Primary Process Code \n",
        " <font color=red>Intendedly deleted </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5c4ujG36rNXK"
      },
      "source": [
        "## The Apache Beam ParDo wrapper"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4b07-fFtjnN"
      },
      "source": [
        "import apache_beam as beam\r\n",
        "\r\n",
        "class OneRowParDo(beam.DoFn):\r\n",
        "    def process(self, element):\r\n",
        "      return processOneMsg(element)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G90bygb6qVBy"
      },
      "source": [
        "## The Beam Pipeline for batch, actually from an CSV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2tjl-SS6qnbj"
      },
      "source": [
        "streamingMode=False\r\n",
        "pubSubTopic=\"projects/grasshopper-298307/topics/L3toL1\"\r\n",
        "\r\n",
        "def run_pipeline():\r\n",
        "    if streamingMode:\r\n",
        "        options = PipelineOptions(args, save_main_session=True, streaming=True)\r\n",
        "    else:\r\n",
        "        options = beam.options.pipeline_options.PipelineOptions(streaming=False)\r\n",
        "\r\n",
        "    p = beam.Pipeline(options=options)\r\n",
        "\r\n",
        "    if streamingMode:\r\n",
        "      read = (\r\n",
        "          p \r\n",
        "          | 'Read from PubSub '>> beam.io.ReadFromPubSub(topic=pubSubTopic)\r\n",
        "      )\r\n",
        "    else:\r\n",
        "      read = (\r\n",
        "          p \r\n",
        "          | 'Reads from csv' >> beam.io.ReadFromText('market_data_v2.csv') \r\n",
        "      )\r\n",
        "    process= (\r\n",
        "        read \r\n",
        "        | 'Structures data' >> beam.ParDo(OneRowParDo())\r\n",
        "        | 'Save L1 data into a file' >> beam.io.WriteToText('L1_market_data',file_name_suffix=\".csv\")\r\n",
        "    )\r\n",
        "    \r\n",
        "    result = p.run()\r\n",
        "    result.wait_until_finish()  # For it to hold the terminal until it finishes\r\n",
        "\r\n",
        "run_pipeline()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMpnUtBSGRB7"
      },
      "source": [
        "## The generated L1 data has been validated with expected_L1_market_data.csv\r\n",
        "\r\n",
        "For any matched row, the rest of columns carry the same values as the expected file.\r\n",
        "\r\n",
        "You may download the generated data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2_kRim40GODL",
        "outputId": "eccf731e-6834-4ac1-df68-e883ca66f335"
      },
      "source": [
        "!ls -l"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 4680\n",
            "-rw-r--r-- 1 root root 2026912 Dec 13 14:00 L1_market_data-00000-of-00001.csv\n",
            "-rw-r--r-- 1 root root 2757472 Dec 13 14:00 market_data_v2.csv\n",
            "drwxr-xr-x 1 root root    4096 Dec  2 22:04 sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gaxLoQTAEFxc"
      },
      "source": [
        "# Implementation Discussion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKt8GUrCET12"
      },
      "source": [
        "## Architecture\r\n",
        "\r\n",
        "Due to the shared order book in the memory, the implementation is not scalable. \r\n",
        "\r\n",
        "Even a STATEFUL Apache Beam doesn't provide the scalability.\r\n",
        "\r\n",
        "In order to be scalable, the shared order book should be put into an external persistent data base, such as Google BigTable.\r\n",
        "\r\n",
        "It depends on the business requirement. If one node can deal with the data in time and the history can be quickly replayed to rebuid the order book. One node model can be accepted. \r\n"
      ]
    }
  ]
}